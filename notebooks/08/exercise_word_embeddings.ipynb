{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../../assets/htwlogo.svg\">\n",
    "\n",
    "# Exercise: Word-Embeddings\n",
    "\n",
    "**Author**: _Erik Rodner_ <br>\n",
    "\n",
    "In this exercise, we will play with word embeddings. The idea was inspired by 3blue1brown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we use is not a modern embedding model, but still fits the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is hard to visualize embedding beyond 3 dimensions, we transform all embedding vectors to three dimensions with a PCA. Alternatively, one could even use a random transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 3\n",
    "pca = PCA(n_components=3)\n",
    "# P = np.random.randn(embedding_size, 100)- just a random transformation as alternative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform some words into the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"sofa\", \"chair\", \"man\", \"boy\", \"girl\", \"crown\", \"throne\", \"fighting\", \"ruling\", \"singing\"]\n",
    "\n",
    "embeddings = np.zeros((len(words), 100))\n",
    "for index, w in enumerate(words):\n",
    "    embeddings[index] = model[w]\n",
    "\n",
    "# transform the embeddings into a lower dimensional space\n",
    "embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2])\n",
    "\n",
    "for i, txt in enumerate(words):\n",
    "    ax.text(embeddings[i, 0], embeddings[i, 1], embeddings[i, 2], txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic operations in embedding space\n",
    "\n",
    "Some embedding spaces have interesting properties, for example that some operations even make sense, let's try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_vector_pointing_to_man = model[\"man\"] - model[\"woman\"]\n",
    "v = gender_vector_pointing_to_man + model[\"aunt\"]\n",
    "model.most_similar(positive=[v], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: more arithmetics\n",
    "\n",
    "Can you find more examples of embedding arithmetics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-exercise-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
